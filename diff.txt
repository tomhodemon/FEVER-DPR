if args.lora:
        lora_config = utils.get_lora_config(args.lora_rank, args.lora_alpha, args.lora_dropout)
    else:
        lora_config = None
    
    bi_encoder = BiEncoder(args.base_model_name, lora_config).to(device)
    bi_encoder.train()

class BiEncoder(nn.Module):
    def __init__(self, model_name, lora_config, p_dropout=0.1):
        super(BiEncoder, self).__init__()
        self.queryEncoder = BertModel.from_pretrained(model_name, add_pooling_layer=False)
        self.passageEncoder = BertModel.from_pretrained(model_name, add_pooling_layer=False)
        if lora_config is not None:
            self.queryEncoder = get_peft_model(self.queryEncoder, lora_config)
            self.queryEncoder.print_trainable_parameters()

            self.passageEncoder = get_peft_model(self.queryEncoder, lora_config)
            self.passageEncoder.print_trainable_parameters()
            
        self.dropout = nn.Dropout(p_dropout)


from peft import get_peft_model


 # LoRA
    parser.add_argument('--lora', action="store_true")
    parser.add_argument('--lora_rank', type=int, default=8)
    parser.add_argument('--lora_alpha', type=float, default=32)
    parser.add_argument('--lora_dropout', type=float, default=0.1)

from peft import LoraConfig
def get_lora_config(lora_rank, lora_alpha, lora_dropout):
    return LoraConfig(
            target_modules=["query", "value"],
            inference_mode=False, 
            r=lora_rank, 
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            bias="none")


## EVAL


if args.lora:
        base_model = AutoModel.from_pretrained("bert-base-cased", add_pooling_layer=False)
        
        queryEncoder = PeftModel.from_pretrained(base_model, args.query_encoder).to(device)
        queryEncoder.eval()

        passageEncoder = PeftModel.from_pretrained(base_model, args.passage_encoder).to(device)
        passageEncoder.eval()

    else:
        queryEncoder = AutoModel.from_pretrained(args.query_encoder, add_pooling_layer=False).to(device)
        queryEncoder.eval()

        passageEncoder = AutoModel.from_pretrained(args.passage_encoder, add_pooling_layer=False).to(device)
        passageEncoder.eval()


    parser.add_argument("--lora", action="store_true")